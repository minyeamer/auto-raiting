{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('korean')\n",
    "from konlpy.tag import Okt\n",
    "import googletrans\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib\n",
    "import time\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>조아요 처음구입 싸게햇어요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>생각보다 잘 안돼요 매지 바른지 하루밖에 안됐는데ㅠㅠ 25천원가량 주고 사기 너무 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>디자인은괜찮은데 상품이 금이가서 교환했는데 두번째받은상품도 까져있고 안쪽에 금이가져...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>기전에 이 제품말고 이마트 트레이더스에서만 팔던 프리미엄 제품을 사용했었습니다. 샘...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>튼튼하고 손목을 잘 받쳐주네요~</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            reviews  target\n",
       "0   0                                     조아요 처음구입 싸게햇어요       2\n",
       "1   1  생각보다 잘 안돼요 매지 바른지 하루밖에 안됐는데ㅠㅠ 25천원가량 주고 사기 너무 ...       1\n",
       "2   2  디자인은괜찮은데 상품이 금이가서 교환했는데 두번째받은상품도 까져있고 안쪽에 금이가져...       2\n",
       "3   3  기전에 이 제품말고 이마트 트레이더스에서만 팔던 프리미엄 제품을 사용했었습니다. 샘...       2\n",
       "4   4                                  튼튼하고 손목을 잘 받쳐주네요~       5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(data=train, x='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨 간 불균형을 조정하기 위해 번역을 활용한 데이터 증강기법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 한글 > 영어 > 한글 역번역\n",
    "- 번역기의 버전 차이인지 실제 웹 상에서의 번역 결과와 googletrans를 통한 번역 결과가 다름\n",
    "- '별로'를 'good'으로 번역하는 등의 이슈가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = googletrans.Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "화면하고 전혀틀림\n",
      "The screen is completely different\n",
      "화면은 완전히 다릅니다\n",
      "--------------------------------------------------\n",
      "배송도 빠르고 눈썹은 괜찮은데 글로가 정말 별로네요ㅜ\n",
      "The delivery is fast and the eyebrows are fine, but the writing is really good.\n",
      "배달은 빠르고 눈썹은 괜찮지 만 글은 정말 좋습니다.\n",
      "--------------------------------------------------\n",
      "재구매 빠른배송 감사합니다 노란색이 더좋았어요\n",
      "Thank you for your repurchase fast delivery. The yellow color was better.\n",
      "재구매 빠른 배송에 감사드립니다.노란색이 더 좋았습니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [11,121,1331]:\n",
    "    original = train['reviews'].iloc[i]\n",
    "    translated = translator.translate(original, dest='en', src='ko').text\n",
    "    result = translator.translate(translated, dest='ko', src='en').text\n",
    "\n",
    "    print(original)\n",
    "    print(translated)\n",
    "    print(result)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. NLPAUG 동의어 변환\n",
    "- WordNet에서 한글 소스가 존재하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red orchard apple tree']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.SynonymAug()\n",
    "text = 'red apple'\n",
    "aug.augment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 역번역 중간에 NLPAUG 적용\n",
    "- 번역된 문장이 올바를 경우에 원본 문장의 감정을 변질시키지 않고 다른 표현으로 변환할 수 있음을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "화면하고 전혀틀림\n",
      "The screen is completely different\n",
      "The blind be completely different\n",
      "블라인드는 완전히 다릅니다\n",
      "--------------------------------------------------\n",
      "배송도 빠르고 눈썹은 괜찮은데 글로가 정말 별로네요ㅜ\n",
      "The delivery is fast and the eyebrows are fine, but the writing is really good.\n",
      "The livery constitute tight and the brow are fine, just the writing is really good.\n",
      "정복은 단단하고 눈썹은 괜찮습니다. 글쓰기 만 정말 좋습니다.\n",
      "--------------------------------------------------\n",
      "재구매 빠른배송 감사합니다 노란색이 더좋았어요\n",
      "Thank you for your repurchase fast delivery. The yellow color was better.\n",
      "Thank you for your repurchase fast delivery. The yellow colouring material be well.\n",
      "재구매 빠른 배송에 감사드립니다.노란색 채색 재료가 잘됩니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [11,121,1331]:\n",
    "    original = train['reviews'].iloc[i]\n",
    "    translated = translator.translate(original, dest='en', src='ko').text\n",
    "    augmented = aug.augment(translated)[0]\n",
    "    result = translator.translate(augmented, dest='ko', src='en').text\n",
    "\n",
    "    print(original)\n",
    "    print(translated)\n",
    "    print(augmented)\n",
    "    print(result)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 토큰 단위로 역번역\n",
    "- 문장 번역보다는 그럴듯한 결과가 나왔지만, 역번역이 여전히 어설프고 번역 속도가 너무 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ko_tokenize(sentence):\n",
    "    okt = Okt()\n",
    "    sentence = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','',sentence)\n",
    "    tokenized = okt.morphs(sentence)\n",
    "    return [word for word in tokenized if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['화면', '하고', '전혀', '틀림']\n",
      "['screen', 'do', 'Not at all', 'error']\n",
      "['화면', '전혀', '오류']\n",
      "--------------------------------------------------\n",
      "['배송', '빠르고', '눈썹', '괜찮은데', '글로', '정말', '별로', '요', 'ㅜ']\n",
      "['shipping', 'Fast', 'Eyebrow', \"It's okay\", 'Writings', 'really', 'not really', 'yo. this', 'sob']\n",
      "['배송', '빠른', '눈썹', '괜찮아요', '진짜', '에야', '디', '흐', '느낌']\n",
      "--------------------------------------------------\n",
      "['재구매', '빠른', '배송', '감사합니다', '노란색', '더', '좋았어요']\n",
      "['Repurchase', 'fast', 'shipping', 'thank you', 'yellow', 'more', 'It was good']\n",
      "['환매', '빠른', '배송', '감사합니다', '노란색', '더', '좋았습니다']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [11,121,1331]:\n",
    "    original = ko_tokenize(train['reviews'].iloc[i])\n",
    "    translated = [translator.translate(token, dest='en', src='ko').text for token in original]\n",
    "    result = [translator.translate(token, dest='ko', src='en').text for token in translated]\n",
    "    result = ko_tokenize(' '.join(result))\n",
    "\n",
    "    print(original)\n",
    "    print(translated)\n",
    "    print(result)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Word2Vec를 활용한 유사어 변환\n",
    "- 외부 데이터를 활용하지 못하는 환경에서는 원본 데이터의 단어를 조합하는 수준에 불과하기 때문에 유의미한 결과를 내지 못할 것이라 판단\n",
    "- 또한 서로 다른 감정의 단어를 묶어서 벡터를 생성할 경우 1점 리뷰에 '감사합니다'라는 단어가 들어가는 등의 예측을 방해하는 노이즈가 발생하는 이슈를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "num_features = 300\n",
    "train_data = [ko_tokenize(data) for data in train['reviews']]\n",
    "model = Word2Vec(sentences=train_data, vector_size=num_features, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['화면', '하고', '전혀', '틀림']\n",
      "['베란다', '계란', '시중', '해서']\n",
      "--------------------------------------------------\n",
      "['배송', '빠르고', '눈썹', '괜찮은데', '글로', '정말', '별로', '요', 'ㅜ']\n",
      "['충전', '빨랐어요', '양도', '기네', '글로', '였어요', '감사합니다', '특히', '손']\n",
      "--------------------------------------------------\n",
      "['재구매', '빠른', '배송', '감사합니다', '노란색', '더', '좋았어요']\n",
      "['빨라요', '사이트', '라서', '거리', '항상', '친절하고', '감사합니다']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [11,121,1331]:\n",
    "    original = ko_tokenize(train['reviews'].iloc[i])\n",
    "    result = list()\n",
    "    for token in original:\n",
    "        synonym = model.wv.most_similar(token)[0]\n",
    "        result.append(synonym[0] if synonym[1] > 0.95 else token)\n",
    "    result = list(set(result))\n",
    "\n",
    "    print(original)\n",
    "    print(result)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 파파고 크롤링을 활용한 한영 번역 후 구글 번역을 통한 역번역\n",
    "- 파파고 API를 활용 시 더욱 빨리 번역을 진행할 수 있지만, 글자 수 단위로 과금이 발생하기에 크롤링으로 번역되지 않는 문장에 한해 시행\n",
    "- 평점 5점에 대한 리뷰의 경우 증강을 진행하지 않을 것이기 때문에 번역할 필요가 없지만, 시간이 남아 같이 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 글자 수: 999702\n"
     ]
    }
   ],
   "source": [
    "print('전체 글자 수:', sum([len(review) for review in train['reviews']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "화면하고 전혀틀림\n",
      "Completely different from the screen\n",
      "화면과 완전히 다릅니다\n",
      "--------------------------------------------------\n",
      "배송도 빠르고 눈썹은 괜찮은데 글로가 정말 별로네요ㅜ\n",
      "The delivery is fast and the eyebrows are okay, but the writing is really bad\n",
      "배달은 빠르고 눈썹은 괜찮지 만 글은 정말 나쁘다\n",
      "--------------------------------------------------\n",
      "재구매 빠른배송 감사합니다 노란색이 더좋았어요\n",
      "Thank you for repurchase and fast shipping I liked yellow more\n",
      "재구매하고 빠른 배송 감사합니다. 나는 노란색 더 좋아했습니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "service = Service(executable_path=ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "src, dest = 'ko', 'en'\n",
    "translate_url = f'https://papago.naver.com/?sk={src}&tk={dest}'\n",
    "\n",
    "driver.get(translate_url)\n",
    "time.sleep(3)\n",
    "\n",
    "for i in [11,121,1331]:\n",
    "    original = train['reviews'].iloc[i]\n",
    "\n",
    "    driver.find_element(By.ID, 'txtSource').clear()\n",
    "    driver.find_element(By.ID, 'txtSource').send_keys(original)\n",
    "\n",
    "    time.sleep(3)\n",
    "    translated = driver.find_element(By.ID, 'txtTarget').text\n",
    "    result = translator.translate(translated, dest=src, src=dest).text\n",
    "\n",
    "    print(original)\n",
    "    print(translated)\n",
    "    print(result)\n",
    "    print('-'*50)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Process\n",
    "1. 파파고 크롤링을 통해 한글 리뷰를 영어로 변환한 데이터프레임 생성\n",
    "2. NLPAUG 라이브러리의 동의어 변형 기법을 활용해 랜덤한 문장을 필요한 수만큼 증강\n",
    "3. 이때, 한글로 완벽하게 번역되지 않는 문장을 제거하면서 목표한만큼의 문장을 생성\n",
    "4. 증강된 영문을 구글 번역을 통해 역번역해 한글 리뷰 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예상 시간: 1184분\n"
     ]
    }
   ],
   "source": [
    "total_delay = sum([np.sqrt(np.log(len(review)))+1 for review in train['reviews']])\n",
    "expected_time = int((3 + total_delay) // 60)\n",
    "print(f'예상 시간: {expected_time}분')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(executable_path=ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "src, dest = 'ko', 'en'\n",
    "translate_url = f'https://papago.naver.com/?sk={src}&tk={dest}'\n",
    "train['translated'] = None\n",
    "translated, error = list(), list()\n",
    "\n",
    "driver.get(translate_url)\n",
    "time.sleep(3)\n",
    "\n",
    "for i, review in enumerate(tqdm_notebook(train['reviews'][11355:]),11355):\n",
    "    try:\n",
    "        driver.find_element(By.ID, 'txtSource').clear()\n",
    "        driver.find_element(By.ID, 'txtSource').send_keys(review)\n",
    "        time.sleep(np.sqrt(np.log(len(review)))+1)\n",
    "        train['translated'][i] = driver.find_element(By.ID, 'txtTarget').text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          25000 non-null  int64 \n",
      " 1   reviews     25000 non-null  object\n",
      " 2   target      25000 non-null  int64 \n",
      " 3   translated  24920 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 중 딜레이가 충분하지 않아 이전에 번역된 문장이 저장되거나, 이모티콘 등으로 인해 번역이 진행되지 않은 경우를 확인   \n",
    "해당하는 행들을 추출하여 파파고 API를 통해 한영 번역 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'X-NCP-APIGW-API-KEY-ID': 'YOUR_CLIENT_ID',\n",
    "    'X-NCP-APIGW-API-KEY': 'YOUR_CLIENT_SECRET'\n",
    "}\n",
    "url = 'https://naveropenapi.apigw.ntruss.com/nmt/v1/translation'\n",
    "errors = dict()\n",
    "\n",
    "untranslated = train[(train['translated'].duplicated()) | (train['translated'].isna())]\n",
    "indices = untranslated.index\n",
    "reviews = untranslated['reviews']\n",
    "\n",
    "for i, review in zip(indices, reviews):\n",
    "    data = 'source=ko&target=en&text=' + urllib.parse.quote(review)\n",
    "    request = urllib.request.Request(url, headers=headers)\n",
    "    response = urllib.request.urlopen(request, data=data.encode('utf-8'))\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read().decode('utf-8')\n",
    "        translated = re.search('\"translatedText\":\"(.+?)\"', response_body).group(1)\n",
    "        train['translated'][i] = translated.encode('utf-8', 'ignore').decode('utf-8').strip()\n",
    "    else:\n",
    "        errors[i] = rescode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역 결과에 포함된 이모티콘 유니코드를 출력 가능한 형태로 변환하기 어려워 문자열 인코딩을 통해 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          25000 non-null  int64 \n",
      " 1   reviews     25000 non-null  object\n",
      " 2   target      25000 non-null  int64 \n",
      " 3   translated  25000 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('translated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['reviews'] = train['reviews'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','')\n",
    "test['reviews'] = test['reviews'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','')\n",
    "\n",
    "train['reviews'] = train['reviews'].str.replace('^ +','')\n",
    "test['reviews'] = test['reviews'].str.replace('^ +','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "train_data = [ko_tokenize(review) for review in train['reviews'].tolist()]\n",
    "train_label = train['target'].tolist()\n",
    "test_data = [ko_tokenize(review) for review in test['reviews'].tolist()]\n",
    "\n",
    "num_aug = train['target'].value_counts().max()\n",
    "for rate in train['target'].unique():\n",
    "    reviews = train[train['target']==rate]['reviews'].tolist()\n",
    "    if len(reviews) == num_aug:\n",
    "        continue\n",
    "\n",
    "    augmented = word2vec_aug(reviews, num_aug)\n",
    "    train_data += augmented\n",
    "    train_label += [rate for _ in range(len(augmented))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'reviews':train_data,'target':train_label}).to_csv('data/tokenized1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(data={'target':train_label}, x='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500 17500 7500 7500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = \\\n",
    "    train_test_split(train_data, train_label, test_size=0.3,\n",
    "                        stratify=train_label, shuffle=True, random_state=0)\n",
    "\n",
    "print(len(train_X), len(train_y), len(val_X), len(val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embedding_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Add & Norm\n",
    "        ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embedding_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 2s 0us/step\n",
      "훈련용 리뷰 개수 : 25000\n",
      "테스트용 리뷰 개수 : 25000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # 빈도수 상위 2만개의 단어만 사용\n",
    "max_len = 200  # 문장의 최대 길이\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\n",
    "print('테스트용 리뷰 개수 : {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32  # 각 단어의 임베딩 벡터의 차원\n",
    "num_heads = 2  # 어텐션 헤드의 수\n",
    "dff = 32  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(max_len,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\n",
    "x = transformer_block(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))\n",
    "\n",
    "print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12978, 3409, 5926, 11988, 16, 16305, 5868, 8297, 13365, 5842, 6136, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_X['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_X['token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_X['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers, losses, optimizers, metrics, callbacks \n",
    "\n",
    "SEQ_LEN = 512 # 최대 token 개수 이상의 값으로 임의로 설정\n",
    "\n",
    "koelectra = ElectraModel.from_pretrained('monologg/koelectra-base-discriminator')\n",
    "\n",
    "input_token_ids   = layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_token_ids')   # tokens_tensor\n",
    "input_masks       = layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')       # masks_tensor\n",
    "input_segments    = layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segments')    # segments_tensor  \n",
    "\n",
    "koelectra_outputs = koelectra([input_token_ids, input_masks, input_segments]) \n",
    "# koelectra_outputs -> 0: 'last_hidden_state' & 1: 'pooler_output' (== applied GlobalAveragePooling1D on 'last_hidden_state')\n",
    "\n",
    "koelectra_outputs = koelectra_outputs[1]\n",
    "koelectra_outputs = layers.Dropout(0.2)(koelectra_outputs)\n",
    "final_output = layers.Dense(units=4, activation='softmax', kernel_initializer=initializers.TruncatedNormal(stddev=0.02), name=\"classifier\")(koelectra_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_token_ids, input_masks, input_segments], \n",
    "                       outputs=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = 'saved_models/'\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "\n",
    "# For custom models, we have to use \"save_weights_only = True\" (or we should implement a \"get_config\" method @ https://j.mp/3ltUibd) \n",
    "callback_checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_path + 'koelectra_weight.h5', # 이번 실습에서 변경되었습니다\n",
    "                                                monitor='val_sparse_categorical_accuracy',\n",
    "                                                save_best_only=True, \n",
    "                                                save_weights_only = True, \n",
    "                                                verbose=1) \n",
    "                                                \n",
    "# Early-stopping for preventing the overfitting\n",
    "callback_earlystop = callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', \n",
    "                                             min_delta=0.0001, # the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "                                             patience=5) #  Number of epochs with no improvement after which training will be stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25분 가량 소요됩니다 (epochs를 4~5 정도로 주더라도 비슷한 수준으로 학습이 가능합니다)\n",
    "\n",
    "history = model.fit(train_X, train_y, validation_split=0.2,\n",
    "                    epochs=10, batch_size=100,\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback_checkpoint, callback_earlystop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('mldl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86ae205601b6d906014fa7892090616f7e1469eb0aa86f06d2d1803a695f1eb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
